{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Data Lake on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure that your AWS credentials are loaded as env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2 pyspark-shell\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create spark session with hadoop-aws package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.json(\"data/log-data/*/*/*.json\")\n",
    "df = spark.read.json(\"data/song-data/*/*/*/*.json\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+------------------+---------------+--------------------+----------------+--------------------+----------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|     artist_location|artist_longitude|         artist_name|  duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+----------+---------+------------------+--------------------+----+\n",
      "|ARSUVLW12454A4C8B8|       35.83073|           Tennessee|       -85.97874|Royal Philharmoni...|  94.56281|        1|SOBTCUI12A8AE48B70|Faust: Ballet Mus...|   0|\n",
      "|ARA04401187B991E6E|       54.99241|Londonderry, Nort...|        -7.31923|JOSEF LOCKE & ORC...| 184.16281|        1|SOXKFTF12A6D4FBF31|Isle Of Innisfree...|   0|\n",
      "|ARXQC081187FB4AD42|       54.31407|                  UK|        -2.23001|William Shatner_ ...|1047.71873|        1|SOXRPUH12AB017F769|Exodus: Part I: M...|   0|\n",
      "|ARWUNH81187FB4A3E0|           null|     Miami , Florida|            null|         Trick Daddy| 227.10812|        1|SOVNKJI12A8C13CB0D|Take It To Da Hou...|2001|\n",
      "|ARNU0OM1187FB3F14A|       32.77815|          Dallas, TX|        -96.7954|Larry Groce/Disne...|  90.04363|        1|SOPEJZP12A8C1369E6|He's Got The Whol...|   0|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+----------+---------+------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "904"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print schema\n",
    "#df.printSchema()\n",
    "#df.show(5)\n",
    "# Number of records\n",
    "#df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+----------------+---------------+--------------------+\n",
      "|         artist_id|         artist_name|artist_longitude|artist_latitude|     artist_location|\n",
      "+------------------+--------------------+----------------+---------------+--------------------+\n",
      "|ARSUVLW12454A4C8B8|Royal Philharmoni...|       -85.97874|       35.83073|           Tennessee|\n",
      "|ARA04401187B991E6E|JOSEF LOCKE & ORC...|        -7.31923|       54.99241|Londonderry, Nort...|\n",
      "|ARXQC081187FB4AD42|William Shatner_ ...|        -2.23001|       54.31407|                  UK|\n",
      "|ARWUNH81187FB4A3E0|         Trick Daddy|            null|           null|     Miami , Florida|\n",
      "|ARNU0OM1187FB3F14A|Larry Groce/Disne...|        -96.7954|       32.77815|          Dallas, TX|\n",
      "+------------------+--------------------+----------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_table = df.select(\"artist_id\", \"artist_name\", \"artist_longitude\", \"artist_latitude\", \"artist_location\")\n",
    "artists_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'artists_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-efac64fd4a3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0martists_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"artists.parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'artists_table' is not defined"
     ]
    }
   ],
   "source": [
    "artists_table.write.parquet(\"artists.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer schema, fix header and separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3a://udacity-dend/pagila/payment/payment.csv\",sep=\";\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the data yourself "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pyspark.sql.functions as F\n",
    "dfPayment = df.withColumn(\"paymen_date\", F.to_timestamp(\"payment_date\"))\n",
    "dfPayment.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPayment = dfPayment.withColumn(\"month\", F.month(\"payment_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer aggregate revenue per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfPayment.createOrReplaceTempView(\"payment\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT month, sum(amount) as revenue\n",
    "FROM payment\n",
    "GROUP BY month\n",
    "ORDER BY revenue desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, DateType\n",
    "paymentSchema = StructType([\n",
    "    StructField(\"payment_id\", IntegerType()),\n",
    "    StructField(\"customer_id\", IntegerType()),\n",
    "    StructField(\"staff_id\", IntegerType()),\n",
    "    StructField(\"rental_id\", IntegerType()),\n",
    "    StructField(\"amount\", DoubleType()),\n",
    "    StructField(\"paymnet_data\", DateType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPaymentWithSchema = spark.csv.read(\"s3a://udacity-dend/pagila/payment/payment.csv\",sep=\";\", Schema=paymentSchema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPaymentWithSchema.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPaymentWithSchema.createOrReplaceTempView(\"payment\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT month(payment_date) as m, sum(amount) as revenue\n",
    "FROM payment\n",
    "GROUP BY m\n",
    "ORDER BY revenue desc\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
