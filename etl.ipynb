{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Lake on S3 with spark code execution on AWS-EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure that your AWS credentials are loaded as env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# To run the code in local\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2 pyspark-shell\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create spark session with hadoop-aws package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load songs data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+------------------+---------------+--------------------+----------------+--------------------+----------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|     artist_location|artist_longitude|         artist_name|  duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+----------+---------+------------------+--------------------+----+\n",
      "|ARSUVLW12454A4C8B8|       35.83073|           Tennessee|       -85.97874|Royal Philharmoni...|  94.56281|        1|SOBTCUI12A8AE48B70|Faust: Ballet Mus...|   0|\n",
      "|ARA04401187B991E6E|       54.99241|Londonderry, Nort...|        -7.31923|JOSEF LOCKE & ORC...| 184.16281|        1|SOXKFTF12A6D4FBF31|Isle Of Innisfree...|   0|\n",
      "|ARXQC081187FB4AD42|       54.31407|                  UK|        -2.23001|William Shatner_ ...|1047.71873|        1|SOXRPUH12AB017F769|Exodus: Part I: M...|   0|\n",
      "|ARWUNH81187FB4A3E0|           null|     Miami , Florida|            null|         Trick Daddy| 227.10812|        1|SOVNKJI12A8C13CB0D|Take It To Da Hou...|2001|\n",
      "|ARNU0OM1187FB3F14A|       32.77815|          Dallas, TX|        -96.7954|Larry Groce/Disne...|  90.04363|        1|SOPEJZP12A8C1369E6|He's Got The Whol...|   0|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+----------+---------+------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "904"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all the json files under song-data repository\n",
    "# since we know the directory structure we can use *\n",
    "df = spark.read.json(\"data/song-data/*/*/*/*.json\")\n",
    "\n",
    "# print schema\n",
    "df.printSchema()\n",
    "# print 5 records\n",
    "df.show(5)\n",
    "# Print total number of records\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+----------+\n",
      "|           song_id|               title|         artist_id|year|  duration|\n",
      "+------------------+--------------------+------------------+----+----------+\n",
      "|SOBTCUI12A8AE48B70|Faust: Ballet Mus...|ARSUVLW12454A4C8B8|   0|  94.56281|\n",
      "|SOXKFTF12A6D4FBF31|Isle Of Innisfree...|ARA04401187B991E6E|   0| 184.16281|\n",
      "|SOXRPUH12AB017F769|Exodus: Part I: M...|ARXQC081187FB4AD42|   0|1047.71873|\n",
      "|SOVNKJI12A8C13CB0D|Take It To Da Hou...|ARWUNH81187FB4A3E0|2001| 227.10812|\n",
      "|SOPEJZP12A8C1369E6|He's Got The Whol...|ARNU0OM1187FB3F14A|   0|  90.04363|\n",
      "+------------------+--------------------+------------------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe songs_table with the required fields from existind dataframe\n",
    "songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\")\n",
    "songs_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the data frame based on year and artist_id and write it to parquet file\n",
    "songs_table.write.partitionBy([\"year\", \"artist_id\"]).parquet(\"songs.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+----------------+---------------+--------------------+\n",
      "|         artist_id|         artist_name|artist_longitude|artist_latitude|     artist_location|\n",
      "+------------------+--------------------+----------------+---------------+--------------------+\n",
      "|ARSUVLW12454A4C8B8|Royal Philharmoni...|       -85.97874|       35.83073|           Tennessee|\n",
      "|ARA04401187B991E6E|JOSEF LOCKE & ORC...|        -7.31923|       54.99241|Londonderry, Nort...|\n",
      "|ARXQC081187FB4AD42|William Shatner_ ...|        -2.23001|       54.31407|                  UK|\n",
      "|ARWUNH81187FB4A3E0|         Trick Daddy|            null|           null|     Miami , Florida|\n",
      "|ARNU0OM1187FB3F14A|Larry Groce/Disne...|        -96.7954|       32.77815|          Dallas, TX|\n",
      "+------------------+--------------------+----------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe artists_table with the required fields from existind dataframe\n",
    "artists_table = df.select(\"artist_id\", \"artist_name\", \"artist_longitude\", \"artist_latitude\", \"artist_location\")\n",
    "artists_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data frame to the parquet file\n",
    "artists_table.write.parquet(\"artists.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load logs data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the json files under logs-data repository\n",
    "# since we know the directory structure we can use *\n",
    "df = spark.read.json(\"data/log-data/*/*/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+\n",
      "|     artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|           song|status|           ts|           userAgent|userId|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+\n",
      "|   Harmonia|Logged In|     Ryan|     M|            0|   Smith|655.77751| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|  Sehr kosmisch|   200|1542241826796|\"Mozilla/5.0 (X11...|    26|\n",
      "|The Prodigy|Logged In|     Ryan|     M|            1|   Smith|260.07465| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|The Big Gundown|   200|1542242481796|\"Mozilla/5.0 (X11...|    26|\n",
      "|      Train|Logged In|     Ryan|     M|            2|   Smith|205.45261| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|       Marry Me|   200|1542242741796|\"Mozilla/5.0 (X11...|    26|\n",
      "|       null|Logged In|    Wyatt|     M|            0|   Scott|     null| free|Eureka-Arcata-For...|   GET|    Home|1.540872073796E12|      563|           null|   200|1542247071796|Mozilla/5.0 (Wind...|     9|\n",
      "|       null|Logged In|   Austin|     M|            0| Rosales|     null| free|New York-Newark-J...|   GET|    Home|1.541059521796E12|      521|           null|   200|1542252577796|Mozilla/5.0 (Wind...|    12|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8056"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records based on NextSong and rename the columns as per requirements\n",
    "df = df.filter(\"page='NextSong'\")\\\n",
    "        .withColumnRenamed(\"userId\", \"user_id\")\\\n",
    "        .withColumnRenamed(\"firstName\", \"first_name\")\\\n",
    "        .withColumnRenamed(\"lastName\", \"last_name\")\\\n",
    "        .withColumnRenamed(\"sessionId\", \"session_id\")\\\n",
    "        .withColumnRenamed(\"userAgent\", \"user_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "|     61|    Samuel| Gonzalez|     M| free|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe users_table with the required fields from existind dataframe\n",
    "users_table = df.select(\"user_id\", \"first_name\", \"last_name\", \"gender\", \"level\")\n",
    "users_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data frame to the parquet file\n",
    "users_table.write.parquet(\"users.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the epoch milliseconds to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pyspark.sql.functions as F\n",
    "\n",
    "# udf method to convert millisecnds to epoch unix time\n",
    "get_timestamp = F.udf(lambda x: int(int(x)/1000))\n",
    "\n",
    "# Add the required columns for time table in the existing data frame\n",
    "df = df.withColumn(\"timestamp\", get_timestamp(\"ts\"))\\\n",
    "        .withColumn(\"datetime\", F.from_unixtime(\"timestamp\", \"MM-dd-yyyy HH:mm:ss\"))\\\n",
    "        .withColumn(\"start_time\", F.to_timestamp(\"datetime\", \"MM-dd-yyyy HH:mm:ss\"))\\\n",
    "        .withColumn(\"month\", F.month(\"start_time\"))\\\n",
    "        .withColumn(\"year\", F.year(\"start_time\"))\\\n",
    "        .withColumn(\"week\", F.weekofyear(\"start_time\"))\\\n",
    "        .withColumn(\"day\", F.dayofmonth(\"start_time\"))\\\n",
    "        .withColumn(\"weekday\", F.dayofweek(\"start_time\"))\\\n",
    "        .withColumn(\"hour\", F.hour(\"start_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+----+----+---+-------+----+\n",
      "|start_time         |month|year|week|day|weekday|hour|\n",
      "+-------------------+-----+----+----+---+-------+----+\n",
      "|2018-11-14 17:30:26|11   |2018|46  |14 |4      |17  |\n",
      "|2018-11-14 17:41:21|11   |2018|46  |14 |4      |17  |\n",
      "|2018-11-14 17:45:41|11   |2018|46  |14 |4      |17  |\n",
      "|2018-11-14 20:44:09|11   |2018|46  |14 |4      |20  |\n",
      "|2018-11-14 22:48:55|11   |2018|46  |14 |4      |22  |\n",
      "+-------------------+-----+----+----+---+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe time_table with the required fields from existind dataframe\n",
    "time_table = df.select(\"start_time\", \"month\", \"year\", \"week\", \"day\", \"weekday\", \"hour\")\n",
    "time_table.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the data frame based on year and month and write it to parquet file\n",
    "time_table.write.partitionBy([\"year\", \"month\"]).parquet(\"time.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+---------+----+------------------+\n",
      "|           song_id|               title| duration|year|         artist_id|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "|SOBTCUI12A8AE48B70|Faust: Ballet Mus...| 94.56281|   0|ARSUVLW12454A4C8B8|\n",
      "|SOVNKJI12A8C13CB0D|Take It To Da Hou...|227.10812|2001|ARWUNH81187FB4A3E0|\n",
      "|SOYVBGZ12A6D4F92A8|Piano Sonata No. ...|221.70077|   0|ARLRWBW1242077EB29|\n",
      "|SODBHKO12A58A77F36|Fingers Of Love (...|335.93424|   0|ARKGS2Z1187FB494B5|\n",
      "|SOGXFIF12A58A78CC4|Hanging On (Mediu...|204.06812|   0|AR5LZJD1187FB4C5E5|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_df = spark.read.parquet(\"songs.parquet\")\n",
    "songs_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create song plays table by joining songs_table and current dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create views to perform sql query\n",
    "songs_df.createOrReplaceTempView(\"songs_data\")\n",
    "df.createOrReplaceTempView(\"logs_data\")\n",
    "\n",
    "# SQL query to join data from different dataframes and create a new data frame from it\n",
    "songplays_table = spark.sql(\"\"\"\n",
    "                    SELECT DISTINCT start_time, user_id, level, song_id, artist_id,\n",
    "                                    session_id, location, user_agent, logs.year, month\n",
    "                    FROM logs_data as logs\n",
    "                    LEFT OUTER JOIN songs_data as songs\n",
    "                    ON logs.song = songs.title\n",
    "                    AND logs.length = songs.duration\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create primary key for the songplays_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-----+-------+---------+----------+--------------------+--------------------+----+-----+------------+\n",
      "|         start_time|user_id|level|song_id|artist_id|session_id|            location|          user_agent|year|month|songplays_id|\n",
      "+-------------------+-------+-----+-------+---------+----------+--------------------+--------------------+----+-----+------------+\n",
      "|2018-11-15 04:08:08|     80| paid|   null|     null|       611|Portland-South Po...|\"Mozilla/5.0 (Mac...|2018|   11|           0|\n",
      "|2018-11-15 05:14:26|     80| paid|   null|     null|       611|Portland-South Po...|\"Mozilla/5.0 (Mac...|2018|   11|           1|\n",
      "|2018-11-15 16:46:20|     44| paid|   null|     null|       637|Waterloo-Cedar Fa...|Mozilla/5.0 (Maci...|2018|   11|           2|\n",
      "|2018-11-20 20:16:43|     80| paid|   null|     null|       774|Portland-South Po...|\"Mozilla/5.0 (Mac...|2018|   11|           3|\n",
      "|2018-11-21 11:29:12|     97| paid|   null|     null|       817|Lansing-East Lans...|\"Mozilla/5.0 (X11...|2018|   11|           4|\n",
      "+-------------------+-------+-----+-------+---------+----------+--------------------+--------------------+----+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a column songplays_id and assign it values using monotonically_increasing_id method\n",
    "songplays_table = songplays_table.withColumn(\"songplays_id\", F.monotonically_increasing_id())\n",
    "\n",
    "songplays_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the data frame based on year and month and write it to parquet file\n",
    "songplays_table.write.partitionBy([\"year\", \"month\"]).parquet(\"songplays.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
